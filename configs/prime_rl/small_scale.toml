# Prime Intellect RL Configuration - Small Scale
# Lightweight configuration for quick experiments or debugging

model = "nanochat-sft"

[env]
id = "harleycooper/nanochatAquaRat"

[env.args]
num_train_examples = 500   # Smaller dataset for faster iteration
num_eval_examples = 100
seed = 42
include_rationale_metadata = true

[trainer.args]
learning_rate = 2e-5
rollouts_per_example = 4   # Fewer rollouts
max_steps = 100            # Shorter training
per_device_train_batch_size = 2
gradient_accumulation_steps = 2
eval_steps = 25
save_steps = 50
logging_steps = 5

temperature = 1.0
top_k = 50
max_new_tokens = 256

weight_decay = 0.0
warmup_steps = 10
max_grad_norm = 1.0

report_to = ["wandb"]
run_name = "prime_rl_small_scale"

[wandb]
project = "nanochat-prime-rl"
tags = ["prime-rl", "nanochat", "aquarat", "small-scale", "debug"]
notes = "Small-scale training for debugging and quick iteration"
