# Prime Intellect RL Configuration - Full Dataset Training
# Configuration for training on the complete AQuA-RAT dataset

model = "nanochat-sft"

[env]
id = "harleycooper/nanochatAquaRat"

[env.args]
num_train_examples = -1    # Use all training examples (~97k)
num_eval_examples = 254
seed = 42
include_rationale_metadata = true

[trainer.args]
learning_rate = 3e-5
rollouts_per_example = 12
max_steps = 2000           # Longer training for full dataset
per_device_train_batch_size = 4
gradient_accumulation_steps = 8  # Larger effective batch size
eval_steps = 200
save_steps = 400
logging_steps = 20

temperature = 1.0
top_k = 50
max_new_tokens = 256

weight_decay = 0.01
warmup_steps = 200
max_grad_norm = 1.0

report_to = ["wandb"]
run_name = "prime_rl_full_dataset"

[wandb]
project = "nanochat-prime-rl"
tags = ["prime-rl", "nanochat", "aquarat", "full-dataset", "production"]
notes = "Full dataset training with complete AQuA-RAT corpus"
